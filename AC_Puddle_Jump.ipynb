{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib tk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import hsv_to_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_range(values, vmin=0, vmax=1):\n",
    "    start_zero = values - np.min(values)\n",
    "    return (start_zero / (np.max(start_zero) + 1e-7)) * (vmax - vmin) + vmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    terrain_color = dict(normal=[127/360, 0, 96/100],\n",
    "                         objective=[26/360, 100/100, 100/100],\n",
    "                         puddle=[247/360, 92/100, 70/100],\n",
    "                         player=[344/360, 93/100, 100/100])\n",
    "        \n",
    "    def __init__(self, jump_success = 0.2):\n",
    "        self.player = None\n",
    "        self._create_grid()  \n",
    "#         self._draw_grid()\n",
    "        self.p_jump = jump_success\n",
    "        self.flag = 0\n",
    "        \n",
    "    def _create_grid(self, initial_grid=None):\n",
    "        self.grid = self.terrain_color['normal'] * np.ones((10, 10, 3))\n",
    "        self._add_objectives(self.grid)\n",
    "        \n",
    "    def _add_objectives(self, grid):\n",
    "        grid[-3, :] = self.terrain_color['puddle']\n",
    "        grid[0, -1] = self.terrain_color['objective']\n",
    "#         grid[-1, -1] = self.terrain_color['objective']\n",
    "        \n",
    "    def _draw_grid(self):\n",
    "        self.fig, self.ax = plt.subplots(figsize=(15, 15))\n",
    "        self.ax.grid(which='minor')       \n",
    "        self.q_texts = [self.ax.text(*self._id_to_position(i)[::-1], '0',\n",
    "                                     fontsize=6, verticalalignment='center', \n",
    "                                     horizontalalignment='center') for i in range(10 * 10)]     \n",
    "         \n",
    "        self.im = self.ax.imshow(hsv_to_rgb(self.grid), cmap='terrain',\n",
    "                                 interpolation='nearest', vmin=0, vmax=1)        \n",
    "        self.ax.set_xticks(np.arange(10))\n",
    "        self.ax.set_xticks(np.arange(10) - 0.5, minor=True)\n",
    "        self.ax.set_yticks(np.arange(10))\n",
    "        self.ax.set_yticks(np.arange(10) - 0.5, minor=True)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.player = (9, 0)\n",
    "        self.flag = 0\n",
    "        return self._position_to_id(self.player)\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Possible actions\n",
    "        #up\n",
    "        if action == 0 and (self.player[0] > 0 and self.player[0]!=8):  \n",
    "            self.player = (self.player[0] - 1, self.player[1])\n",
    "        elif action == 0 and self.player[0] == 8:\n",
    "            self.player = (9, 0)\n",
    "            \n",
    "        #down\n",
    "        if action == 1 and (self.player[0] < 9 and self.player[0]!=6): \n",
    "            self.player = (self.player[0] + 1, self.player[1])\n",
    "        elif action == 1 and self.player[0] == 6:\n",
    "            self.player = (9, 0)\n",
    "            \n",
    "        #right\n",
    "        if action == 2 and self.player[1] < 9: \n",
    "            self.player = (self.player[0], self.player[1] + 1)\n",
    "        \n",
    "        \n",
    "        #left\n",
    "        if action == 3 and self.player[1] > 0:  \n",
    "            self.player = (self.player[0], self.player[1] - 1)\n",
    "            \n",
    "        #jump\n",
    "        if action == 4 and self.player[0] == 8:\n",
    "            if np.random.random() < self.p_jump:\n",
    "                self.player = (6, self.player[1])\n",
    "                if self.flag == 0:\n",
    "                    self.flag = 1\n",
    "        elif action == 4 and self.player[0] == 6:\n",
    "            if np.random.random() < self.p_jump:\n",
    "                self.player = (8, self.player[1])\n",
    "        \n",
    "        # Rules\n",
    "        if self.player == (0,9):\n",
    "            reward = 1000\n",
    "            done = True\n",
    "        else:\n",
    "            reward = -0.05\n",
    "            done = False\n",
    "            \n",
    "        ## partially observable state 8 = state 19\n",
    "        if self._position_to_id(self.player) == 19:\n",
    "            next_position = 8\n",
    "        else:\n",
    "            next_position = self._position_to_id(self.player)\n",
    "        return next_position, reward, done\n",
    "    \n",
    "    def _position_to_id(self, pos):\n",
    "        ''' Maps a position in x,y coordinates to a unique ID '''\n",
    "        return pos[0] * 10 + pos[1]\n",
    "    \n",
    "    def _id_to_position(self, idx):\n",
    "        return (idx // 10), (idx % 10)\n",
    "        \n",
    "    def render(self, q_values=None, q_counts=None, action=None, max_q=False, colorize_q=False, show_count=False):\n",
    "        assert self.player is not None, 'You first need to call .reset()'  \n",
    "        \n",
    "        if colorize_q:\n",
    "            assert q_values is not None, 'q_values must not be None for using colorize_q'            \n",
    "            grid = self.terrain_color['normal'] * np.ones((10, 10, 3))\n",
    "            values = change_range(np.max(q_values, -1)).reshape(10, 10)\n",
    "            grid[:, :, 1] = values\n",
    "            self._add_objectives(grid)\n",
    "        else:            \n",
    "            grid = self.grid.copy()\n",
    "            \n",
    "        grid[self.player] = self.terrain_color['player']       \n",
    "        self.im.set_data(hsv_to_rgb(grid))\n",
    "               \n",
    "        if q_values is not None:\n",
    "            xs = np.repeat(np.arange(10), 10)\n",
    "            ys = np.tile(np.arange(10), 10)  \n",
    "            \n",
    "            for i, text in enumerate(self.q_texts):\n",
    "                if max_q:\n",
    "                    q = max(q_values[i])    \n",
    "                    txt = '{:.2f}'.format(q)\n",
    "                    text.set_text(txt)\n",
    "                elif show_count:                \n",
    "                    actions = ['U', 'D', 'R', 'L', 'J']\n",
    "                    txt = '\\n'.join(['{}: {:.2f}'.format(k, q) for k, q in zip(actions, q_counts[i])])\n",
    "                    text.set_text(txt)\n",
    "                else:\n",
    "                    actions = ['U', 'D', 'R', 'L', 'J']\n",
    "                    txt = '\\n'.join(['{}: {:.2f}'.format(k, q) for k, q in zip(actions, q_values[i])])\n",
    "                    text.set_text(txt)\n",
    "                \n",
    "        if action is not None:\n",
    "            self.ax.set_title(action, color='r', weight='bold', fontsize=32)\n",
    "\n",
    "        plt.pause(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "UP = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "LEFT = 3\n",
    "Jump = 4\n",
    "actions = ['UP', 'DOWN', 'RIGHT', 'LEFT', 'JUMP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld(jump_success=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of states in simply the number of \"squares\" in our grid world, in this case 4 * 12\n",
    "num_states = 10 * 10\n",
    "# We have 4 possible actions, up, down, right and left\n",
    "num_actions = 5\n",
    "\n",
    "q_values = np.zeros((num_states, num_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>up</th>\n",
       "      <th>down</th>\n",
       "      <th>right</th>\n",
       "      <th>left</th>\n",
       "      <th>jump</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>States</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         up   down  right  left  jump\n",
       "States                               \n",
       "0        0.0   0.0    0.0   0.0   0.0\n",
       "1        0.0   0.0    0.0   0.0   0.0\n",
       "2        0.0   0.0    0.0   0.0   0.0\n",
       "3        0.0   0.0    0.0   0.0   0.0\n",
       "4        0.0   0.0    0.0   0.0   0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(q_values, columns=[' up ', 'down', 'right', 'left', 'jump'])\n",
    "df.index.name = 'States'\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manh_d(s1, s2):\n",
    "    return abs(s1[0]-s2[0]) + abs(s1[1]-s2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PBRS(state, u0=10, u1=100):\n",
    "    s = (state // 10), (state % 10)\n",
    "    if s[0] > 7:\n",
    "            phi = u0/5\n",
    "    elif s[0] < 7:\n",
    "            phi = u1/5\n",
    "    else:\n",
    "        phi = 0\n",
    "    return phi*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PBA(state, action, u0=10, u1=100):\n",
    "    s = (state // 10), (state % 10)\n",
    "    if s[0] > 7:\n",
    "        if s[0] == 8:\n",
    "            if action == 4:\n",
    "                phi = u0*0.6\n",
    "            else:\n",
    "                phi = u0*0.1\n",
    "        else:\n",
    "            phi = u0*0.2\n",
    "\n",
    "            \n",
    "    elif s[0] < 7:\n",
    "#         phi = u1*(-manh_d(s_p, c2) + manh_d(s, c2))\n",
    "        if s[0] != 0:\n",
    "            if s[1] != 9:\n",
    "                if action in [0,2]:\n",
    "                    phi = u1*0.7/2\n",
    "                else:\n",
    "                    phi = u1*0.1\n",
    "            else:\n",
    "                if action == 0:\n",
    "                    phi = u1*0.6\n",
    "                else:\n",
    "                    phi = u1*0.1\n",
    "        else:\n",
    "            if action == 2:\n",
    "                phi = u1*0.6\n",
    "            else:\n",
    "                phi = u1*0.1\n",
    "    else:\n",
    "        phi = 0\n",
    "    \n",
    "    return phi*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def egreedy_policy(q_values, state, epsilon=0.1):\n",
    "    ''' \n",
    "    Choose an action based on a epsilon greedy policy.    \n",
    "    A random action is selected with epsilon probability, else select the best action.    \n",
    "    '''\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.choice(5)\n",
    "    else:\n",
    "        return np.argmax(q_values[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(q_values, state, epsilon=0.1):\n",
    "    ''' \n",
    "    Choose an action based on a epsilon greedy policy.    \n",
    "    A random action is selected with epsilon probability, else select the best action.    \n",
    "    '''\n",
    "    prob = np.exp(q_values[state])/np.sum(np.exp(q_values[state]))\n",
    "    return int(np.random.choice(len(prob), 1, p=prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clamp(n, minn=-500, maxn=500):\n",
    "    return max(min(maxn, n), minn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Actor_critic(env, adv = 0, num_episodes=1000, learning_rate_p=0.001, learning_rate_q=0.05, gamma=1.0):\n",
    "    q_values = np.zeros((num_states, num_actions))\n",
    "    policy_para = np.zeros((num_states, num_actions))\n",
    "    v_values = np.zeros((num_states,1))\n",
    "    q_counts = np.zeros((num_states, num_actions))\n",
    "    ep_rewards_train = []\n",
    "    ep_rewards_test = []\n",
    "    ep_steps_train = []\n",
    "    ep_steps_test = []\n",
    "    \n",
    "    ep_next_action_pred = []\n",
    "    \n",
    "    for current_episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        state_p = state\n",
    "        action_p = 0\n",
    "        done = False\n",
    "        reward_sum_train = 0\n",
    "        reward_sum_test = 0\n",
    "        current_steps_train = 0\n",
    "        current_steps_test = 0\n",
    "        \n",
    "        ep_obs = []\n",
    "        ep_as = []\n",
    "        ep_rs = []\n",
    "        next_action_save = []\n",
    "        action_save = []\n",
    "#         next_action = choose_action(policy_para, state)\n",
    "        while not done:            \n",
    "            # Choose action\n",
    "            # proposed\n",
    "            \n",
    "\n",
    "            action = choose_action(policy_para, state)\n",
    "#             action = next_action\n",
    "            \n",
    "            # Do the action\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            next_action = choose_action(policy_para, next_state)\n",
    "        \n",
    "            ep_obs.append(state)\n",
    "            ep_as.append(action)\n",
    "            ep_rs.append(reward)\n",
    "            \n",
    "            next_action_save.append(next_action)\n",
    "            action_save.append(action)\n",
    "            \n",
    "            ## update Q\n",
    "            if adv == 1:\n",
    "\n",
    "                td_target = reward + gamma*PBRS(next_state, 10, 100) \\\n",
    "                                    - PBRS(state, 10, 100) + gamma * v_values[next_state]\n",
    "                td_error = td_target - v_values[state]\n",
    "                v_values[state] += learning_rate_q * td_error\n",
    "            \n",
    "                expected_reward = td_error\n",
    "\n",
    "                log_grad = (1-np.exp(policy_para[state][action])/np.sum(np.exp(policy_para[state])))\n",
    "                policy_para[state][action] = policy_para[state][action] + \\\n",
    "                                                                learning_rate_p*expected_reward*log_grad\n",
    "            elif adv == 2:\n",
    "##########################################look back\n",
    "                td_target = reward + PBA(state, action, 10, 100) \\\n",
    "                                    - PBA(state_p, action_p, 10, 100)/gamma + gamma * v_values[next_state]\n",
    "                td_error = td_target - v_values[state]\n",
    "                v_values[state] += learning_rate_q * td_error\n",
    "                \n",
    "                expected_reward = td_error\n",
    "    \n",
    "                log_grad = (1-np.exp(policy_para[state][action])/np.sum(np.exp(policy_para[state])))\n",
    "                policy_para[state][action] = policy_para[state][action] + \\\n",
    "                                                                learning_rate_p*expected_reward*log_grad\n",
    "##########################################look forward\n",
    "#                 td_target = reward + gamma*PBA(next_state, next_action, 10, 100) \\\n",
    "#                                     - PBA(state, action, 10, 100) + gamma * v_values[next_state]\n",
    "#                 td_error = td_target - v_values[state]\n",
    "#                 v_values[state] += learning_rate_q * td_error\n",
    "                \n",
    "#                 expected_reward = td_error + PBA(state, action, 10, 100) \n",
    "    \n",
    "#                 log_grad = (1-np.exp(policy_para[state][action])/np.sum(np.exp(policy_para[state])))\n",
    "#                 policy_para[state][action] = policy_para[state][action] + \\\n",
    "#                                                                 learning_rate_p*expected_reward*log_grad\n",
    "            \n",
    "\n",
    "            else:\n",
    "                \n",
    "                td_target = reward + gamma * v_values[next_state]\n",
    "                td_error = td_target - v_values[state]\n",
    "                v_values[state] += learning_rate_q * td_error\n",
    "\n",
    "                ##update policy\n",
    "                expected_reward = td_error\n",
    "                log_grad = (1-np.exp(policy_para[state][action])/np.sum(np.exp(policy_para[state])))\n",
    "                policy_para[state][action] = policy_para[state][action] + \\\n",
    "                                                                learning_rate_p*expected_reward*log_grad\n",
    "            \n",
    "            \n",
    "            reward_sum_train += reward\n",
    "            current_steps_train += 1\n",
    "                        \n",
    "            state_p = state    \n",
    "            action_p = action\n",
    "            state = next_state\n",
    "            \n",
    "            if current_steps_train>1000:\n",
    "                break\n",
    "                \n",
    "#             if done:\n",
    "#                 print(current_steps_train, current_episode)\n",
    "            \n",
    "        ep_rewards_train.append(reward_sum_train)\n",
    "\n",
    "        ep_next_action_pred.append(sum(np.array(action_save[1:] ) \n",
    "                                       == np.array(next_action_save[:-1]))/len(action_save[1:]))\n",
    "    return ep_rewards_train, q_values, ep_next_action_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Mean rewards: 973.894125\n"
     ]
    }
   ],
   "source": [
    "ep_reward_train_store = []\n",
    "ep_next_action_pred_store = []\n",
    "env = GridWorld(jump_success=0.2)\n",
    "    \n",
    "    \n",
    "# @parameter adv: \n",
    "# 0: no advice\n",
    "# 1: Uniform SAS\n",
    "# 2: NonUniform SAS \n",
    "for ii in range(10):\n",
    "    ep_rewards_train, q_values, ep_next_action_pred = Actor_critic(env, adv = 2, num_episodes=200, \\\n",
    "                                              learning_rate_p=0.001, learning_rate_q=0.2)\n",
    "    ep_reward_train_store.append(ep_rewards_train)\n",
    "    ep_next_action_pred_store.append(ep_next_action_pred)\n",
    "    print(ii)\n",
    "    \n",
    "avg_reward_train = np.mean(np.array(ep_reward_train_store),axis=0)\n",
    "mean_reward_train = [np.mean(avg_reward_train)] * len(avg_reward_train)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel('Episodes')\n",
    "ax.set_ylabel('Rewards')\n",
    "ax.plot(avg_reward_train)\n",
    "ax.plot(mean_reward_train, 'g--')\n",
    "ax.set_xlim([0,200])\n",
    "\n",
    "\n",
    "print('Mean rewards: {}'.format(mean_reward_train[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
